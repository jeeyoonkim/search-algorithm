{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39c3fb50e0b8b6fc9660e4c2fee09c42",
     "grade": false,
     "grade_id": "cell-05fb407e20c068e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 4: \"Search your transcripts. You will know it to be true.\"\n",
    "\n",
    "Â© Cristian Danescu-Niculescu-Mizil 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://imgs.xkcd.com/comics/mainly_known_for.png\" alt=\"xkcd Mainly Known For: Oh sure, I know Keira Knightly, from the first movie in that series by The Land Before Time producer. You know, the franchise with the guy from Jurassic Park and Ghostwriter, and script work by Billie Lourd's mom?\" width=\"650\"/><br/>(source: <a href=\"https://xkcd.com/2621\">https://xkcd.com/2621</a>)\n",
    "</p>\n",
    "\n",
    "## CS/INFO 4300 Language and Information<a class=\"anchor\" id=\"guidelines\"></a>\n",
    "\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "**DUE**: February 22, 2023 (Wednesday), 11:59pm\n",
    "    \n",
    "</font>\n",
    "\n",
    "<font color='purple'> \n",
    "    \n",
    "**EXTRA CREDIT QUESTION** due: February 24th, 2023 (Friday), 11:59pm (separate CMS submission, same notebook)\n",
    "\n",
    "</font>\n",
    "\n",
    "This is an **individual** assignment. \n",
    "\n",
    "If you use any outside sources (e.g. research papers, StackOverflow) please list your sources.\n",
    "\n",
    "Need to find out the full name of \"steve pixar guy\"? Or perhaps you want to know the identity of \"song guy from labyrinth\"? Sounds like what you need is a system that can efficiently search through large collections of documents!\n",
    "\n",
    "In class, we have seen a number of techniques useful for searching through collections of documents, including edit distance and cosine similarity. Our overall goal in this assignment is to apply these techniques to build a system that efficiently searches for documents similar to a query in large data sets. We will explore the tradeoffs of information retrieval systems by finding newspaper quotes from \"Keeping Up With The Kardashians\".\n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "All cells that contain the blocks that read `# YOUR CODE HERE` are editable and are to be completed to ensure you pass the test-cases. Make sure to write your code where indicated.\n",
    "\n",
    "All cells that read `YOUR ANSWER HERE` are free-response cells that are editable and are to be completed.\n",
    "\n",
    "You may use any number of notebook cells to explore the data and test out your functions, although you will only be graded on the solution itself.\n",
    "\n",
    "You are unable to modify the read-only cells and should never delete any of the given cells.\n",
    "\n",
    "You should also use Markdown cells to explain your code and discuss your results when necessary.\n",
    "Instructions can be found [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "All floating point values should be printed with **2 decimal places** precision. You can do so using the built-in round function.\n",
    "\n",
    "Unless otherwise stated, no cell in this assignment should take longer than **1 second** to run. If a cell takes longer than 1 second to run, it will be marked as incorrect.\n",
    "\n",
    "**Grading**\n",
    "\n",
    "For code-completion questions you will be graded on passing the public test cases we have included, as well as any hidden test cases that we have supplemented to ensure that your logic is correct.\n",
    "\n",
    "For free-response questions you will be manually graded on the quality of your answer.\n",
    "\n",
    "**Learning Objectives**\n",
    "- Examine how edit distance can be applied as a search metric\n",
    "- Develop an understanding of the inverted index and its applications\n",
    "- Explore use cases of boolean search\n",
    "- Examine how the inverted index can be used to efficiently compute IDF values\n",
    "- Introduce cosine similarity as an efficient search model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Contents  <a class=\"anchor\" id=\"contents\"></a>\n",
    "\n",
    "* [Instructions and guidelines](#guidelines)\n",
    "* [Contents](#contents)\n",
    "* [Imports and Motivations](#imports)\n",
    "\n",
    "\n",
    "* [[**Minimum Edit Distance**]](#part1)\n",
    "  * [Imports and data](#0)\n",
    "  * [[1](#q1a)] Levenshtein Edit Distance\n",
    "    * [[1a](#q1a)] Levenshtein Edit Distance\n",
    "    * [[1b](#q1b)] Query Discussion and Analysis\n",
    "  * [[2](#q2a)] Change Levenshtein Edit Distance Costs\n",
    "    * [[2a](#q2a)] Changing the costs\n",
    "    * [[2b](#q2b)] Query Discussion and Analysis\n",
    "    \n",
    "* [[**Cosine Similarity**]](#part2)\n",
    "  * [[3](#q3)] Inverted Index\n",
    "    * [[3](#q3)] Write a function to construct the inverted index\n",
    "  * [[4](#q4a)] Inverted Index and Boolean Search\n",
    "    * [[4a](#q4a)] Using the inverted index for boolean search\n",
    "    * [[4b](#q4b)] Using the inverted index for boolean search\n",
    "  * [[5](#q5)] Inverted Index and IDF\n",
    "    * [[5](#q5)] Compute IDF using the inverted index\n",
    "  * [[6](#q6)] Inverted Index and Norm of Document\n",
    "    * [[6](#q6)] Compute the norm of each document using the inverted index\n",
    "  * [[7](#q7)] Inverted Index and Dot Product\n",
    "    * [[7](#q7)] Efficiently compute the dot product using the inverted index\n",
    "  * [[8](#q8)] Similar Message Finder\n",
    "    * [[8a](#q8)] Find the most similar messages to the quotes\n",
    "    * [[8b](#q8b)] Find the most similar messages to the quotes analysis\n",
    "    \n",
    "* [[**Extra Credit (optional)**]](#ec)  \n",
    "  * [[9](#q9)] Finding your own similarity metric\n",
    "\n",
    "\n",
    "(For those curious, the rendering of backlink to \"contents\" from each section, i.e., the right corner [[contents](#contents)] link, is only accessible via .ipynb notebook, it causes unexpected behavior when viewed in .html.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b513f1577dbc3c3fe7da1cc2fb78803b",
     "grade": false,
     "grade_id": "cell-80fc97f06dc7715b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c2b0f8d6bb1ec27746c1dfd4fa646a3",
     "grade": false,
     "grade_id": "cell-977931635a8ef8aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n"
     ]
    }
   ],
   "source": [
    "with open(\"kardashian-transcripts.json\", \"r\") as f:\n",
    "    transcripts = json.load(f)\n",
    "print(len(transcripts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Most Similar Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The tabloids have been going crazy over our stars! The press took some  quotes from the show, including:\n",
    "       \n",
    " - *\"It's like a bunch of people running around talking about nothing.\"*\n",
    " - *\"Never say to a famous person that this possible endorsment would bring 'er to the spot light.\"*\n",
    " - *\"Your yapping is making my head ache!\"*\n",
    " - *\"I'm going to Maryland, did I tell you?\"*\n",
    " \n",
    "We need to find out who said each of these, and in which episode. But since we're information scientists, that's not enough. We want to build an efficient search engine for retrieving where such quotes come from in the future.\n",
    "\n",
    "What makes this difficult is that journalists often modify the quotes, so exact matching will not always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11948a015dd145cd3bc1683d2a46108",
     "grade": false,
     "grade_id": "cell-ef81cb8deedd1818",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "flat_msgs = [m for transcript in transcripts for m in transcript]\n",
    "queries = [u\"It's like a bunch of people running around talking about nothing.\",\n",
    "           u\"Never say to a famous person that this possible endorsment would bring 'er to the spot light.\",\n",
    "           u\"Your yapping is making my head ache!\",\n",
    "           u\"I'm going to Maryland, did I tell you?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "<p style='text-align:right; font-size:11px'>[[contents](#contents)]</p>\n",
    "\n",
    "## Minimum Edit Distance<a class=\"anchor\" id=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1a: Levenshtein Edit Distance (Code Completion)<a class=\"anchor\" id=\"q1a\"></a>\n",
    "\n",
    "From class and previous assignments, you have learned several methods of comparing similarities between text. Here, we will now first try using the edit distance metric (Levenshtein) to see how it compares the query text with our dataset. For each query, we will need to loop over each message, and compute the edit distance each time, there are no obvious shortcuts. (The code will take some time to run!)\n",
    "\n",
    "We've provided you with code to compute the edit matrix that we discussed in class. You will need to complete the code stub below. (You can refer to the worksheet we used in class if you need a refresher on how this works.)\n",
    "\n",
    "Note that instead of directly coding the insertion, deletion, and substitution costs into the edit matrix code, we've designed the code to take three *functions* as parameters to compute each cost. You'll see why we did it this way once you get to the next question! As in previous assignments, make sure to use these parameters; you will lose points if you hard-code the costs or reference global variables!\n",
    "\n",
    "Note: It is ok for there to be duplicates in your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9232d2db5c42c9a7761adf46e6eb1fd",
     "grade": false,
     "grade_id": "edit_distance",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insertion_cost(message, j):\n",
    "    return 1\n",
    "\n",
    "def deletion_cost(query, i):\n",
    "    return 1\n",
    "\n",
    "def substitution_cost(query, message, i, j):\n",
    "    if query[i-1] == message[j-1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def edit_matrix(query, message, ins_cost_func, del_cost_func, sub_cost_func):\n",
    "    \"\"\" Calculates the edit matrix\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query: query string,\n",
    "        \n",
    "    message: message string,\n",
    "    \n",
    "    ins_cost_func: function that returns the cost of inserting a letter,\n",
    "    \n",
    "    del_cost_func: function that returns the cost of deleting a letter,\n",
    "    \n",
    "    sub_cost_func: function that returns the cost of substituting a letter,\n",
    "    \n",
    "    Returns:\n",
    "        edit matrix {(i,j): int}\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(query) + 1\n",
    "    n = len(message) + 1\n",
    "\n",
    "    chart = {(0, 0): 0}\n",
    "    for i in range(1, m): \n",
    "        chart[i,0] = chart[i-1, 0] + del_cost_func(query, i) \n",
    "    for j in range(1, n): \n",
    "        chart[0,j] = chart[0, j-1] + ins_cost_func(message, j)\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            chart[i, j] = min(\n",
    "                chart[i-1, j] + del_cost_func(query, i),\n",
    "                chart[i, j-1] + ins_cost_func(message, j),\n",
    "                chart[i-1, j-1] + sub_cost_func(query, message, i, j)\n",
    "            )\n",
    "    return chart\n",
    "\n",
    "def edit_distance(query, message, ins_cost_func, del_cost_func, sub_cost_func):\n",
    "    \"\"\" Finds the edit distance between a query and a message using the edit matrix\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query: query string,\n",
    "        \n",
    "    message: message string,\n",
    "    \n",
    "    ins_cost_func: function that returns the cost of inserting a letter,\n",
    "    \n",
    "    del_cost_func: function that returns the cost of deleting a letter,\n",
    "    \n",
    "    sub_cost_func: function that returns the cost of substituting a letter,\n",
    "    \n",
    "    Returns:\n",
    "        edit cost (int)\n",
    "    \"\"\"\n",
    "        \n",
    "    query = query.lower()\n",
    "    message = message.lower()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    d = edit_matrix(query, message,ins_cost_func, del_cost_func, sub_cost_func)\n",
    "    \n",
    "    return d[len(query), len(message)]\n",
    "\n",
    "def edit_distance_search(query, msgs, ins_cost_func, del_cost_func, sub_cost_func):\n",
    "    \"\"\" Edit distance search\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query: string,\n",
    "        The query we are looking for.\n",
    "        \n",
    "    msgs: list of dicts,\n",
    "        Each message in this list has a 'text' field with\n",
    "        the raw document.\n",
    "        \n",
    "    ins_cost_func: function that returns the cost of inserting a letter,\n",
    "    \n",
    "    del_cost_func: function that returns the cost of deleting a letter,\n",
    "    \n",
    "    sub_cost_func: function that returns the cost of substituting a letter,\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    result: list of (score, message) tuples.\n",
    "        The result list is sorted by score such that the closest match\n",
    "        is the top result in the list.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    returnList = []\n",
    "    \n",
    "    for msg in msgs:\n",
    "        score = edit_distance(query, msg['text'],ins_cost_func, del_cost_func, sub_cost_func)\n",
    "        returnList.append( (score, msg) )\n",
    "        \n",
    "    returnList.sort(key=lambda x:x[0])\n",
    "    \n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Autograder tests for this function may take a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b42bed37939333f10bcaf8ea94506864",
     "grade": true,
     "grade_id": "edit_distance_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "score, _ = edit_distance_search(queries[1], flat_msgs, insertion_cost, deletion_cost, substitution_cost)[0]\n",
    "assert score == 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the edit_distance_search function is completed you can run the lines below to print out the best matches for each query string. **NOTE**: This cell will take a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c69f17358aae909186db7d2dbcbcc8ee",
     "grade": false,
     "grade_id": "cell-c469967506e9e184",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################\n",
      "It's like a bunch of people running around talking about nothing.\n",
      "#################################################################\n",
      "[0.00] BRUCE: It's like a bunch of people running around talking about nothing.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[33.00] KRIS: It's not a bunch of teenagers running around.\n",
      "\t(Keeping Up With the Kardashians - Kris ``The Cougar'' Jenner)\n",
      "[35.00] KHLOE: It's like, what are you talking about?\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[37.00] KIM: It's like it has separation anxiety or something.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[37.00] KHLOE: It's like I want to learn how to do that.\n",
      "\t(Keeping Up With the Kardashians - Distance Makes the Heart Grow Fonder)\n",
      "[37.00] KOURTNEY: It's like an explosion in your pantyhose.\n",
      "\t(Keeping Up With the Kardashians - I'd Rather Go Naked... Or Shopping)\n",
      "[38.00] ROB: I have a bunch of connections in the industry.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[38.00] ROB: I have a bunch of connections in the industry.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[38.00] BRUCE: That's why you're running around wearing black.\n",
      "\t(Keeping Up With the Kardashians - I'd Rather Go Naked... Or Shopping)\n",
      "[38.00] BRUCE: That's why you're running around wearing black.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "\n",
      "#############################################################################################\n",
      "Never say to a famous person that this possible endorsment would bring 'er to the spot light.\n",
      "#############################################################################################\n",
      "[42.00] SIMON: You don't tell an international celebrity that this possible endorsement could bring her back into the spotlight.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[59.00] KRIS: You need to trust that I actually know what I'm doing at some point.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[60.00] BRUCE: You know, I've seen her do this before, and it's just not right.\n",
      "\t(Keeping Up With the Kardashians - Baby Blues)\n",
      "[60.00] ALEX: I feel like people want to see you, like, back in the spotlight.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[60.00] KHLOE: Trust me, this is the most entertaining part of the entire night.\n",
      "\t(Keeping Up With the Kardashians - Birthday Suit)\n",
      "[60.00] ROB: I clean all the counters at like from the floors to the bathtub to the toilet.\n",
      "\t(Keeping Up With the Kardashians - Kim's Calendar for Reggie)\n",
      "[60.00] KIM: I didn't want to go on this trip and I'm not talking to them at all.\n",
      "\t(Keeping Up With the Kardashians - Kardashian Civil War)\n",
      "[60.00] ELIZABETH: I reminded you this morning that you needed to bring the designs.\n",
      "\t(Keeping Up With the Kardashians - Family vs. Money)\n",
      "[60.00] KRIS: Okay. If it was you against the other three, I would have felt the same way.\n",
      "\t(Keeping Up With the Kardashians - Kardashian Civil War)\n",
      "[60.00] DAVID: We're gonna take a look at these Polaroids, and then we'll be right with you.\n",
      "\t(Keeping Up With the Kardashians - Khloe's Blind Dates)\n",
      "\n",
      "####################################\n",
      "Your yapping is making my head ache!\n",
      "####################################\n",
      "[15.00] KIM: Your yappity voice is giving me a headache!\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[15.00] KIM: Your yappity voice is giving me a headache!\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[20.00] KIM: You want to make peace?\n",
      "\t(Keeping Up With the Kardashians - Family vs. Money)\n",
      "[21.00] BRUCE: Everything's going to be fine.\n",
      "\t(Keeping Up With the Kardashians - The Missing Ring)\n",
      "[21.00] KHLOE: You are walking me me down...\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[21.00] KHLOE: You are walking me me down...\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[21.00] KRIS: You're going to make a scene.\n",
      "\t(Keeping Up With the Kardashians - What's Yours Is Mine)\n",
      "[22.00] BRUCE: You're diving in for the cash?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[22.00] KHLOE: Your secret is safe with me.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[22.00] BRUCE: You're diving in for   the  cash?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "\n",
      "######################################\n",
      "I'm going to Maryland, did I tell you?\n",
      "######################################\n",
      "[18.00] LAMAR: I'm going to go play with Rob.\n",
      "\t(Keeping Up With the Kardashians - Blind Date)\n",
      "[19.00] KHLOE: I'm going to move to New York.\n",
      "\t(Keeping Up With the Kardashians - Distance Makes the Heart Grow Fonder)\n",
      "[19.00] SCOTT: I'm going to have six or seven?\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[19.00] KHLOE: I'm going to move to New York.\n",
      "\t(Keeping Up With the Kardashians - Free Khloe)\n",
      "[19.00] SCOTT: I'm going to have six or seven?\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[19.00] KRIS: I'm going to lunch with Lisa.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[20.00] FRIEND: I'm going to get canned inside.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[20.00] FRIEND: I'm going to get canned inside.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[20.00] ROB: I'm going to tell her.\n",
      "\t(Keeping Up With the Kardashians - Meet the Kardashians)\n",
      "[20.00] LAMAR: I'm going to need it.\n",
      "\t(Keeping Up With the Kardashians - I Want Your Sex)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note: this will take about 1-2 minutes to execute\n",
    "top_10 = []\n",
    "for query in queries:\n",
    "    print(\"#\" * len(query))\n",
    "    print(query)\n",
    "    print(\"#\" * len(query))\n",
    "\n",
    "    for score, msg in edit_distance_search(query, flat_msgs, insertion_cost, deletion_cost, substitution_cost)[:10]:\n",
    "        print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
    "            score,\n",
    "            msg['speaker'],\n",
    "            msg['text'],\n",
    "            msg['episode_title']))\n",
    "        top_10.append(msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1b: Query Discussion and Analysis (Free Response)<a class=\"anchor\" id=\"q1b\"></a>\n",
    "\n",
    "What do you notice about the search above?\n",
    "1. Discuss why it worked, or why it might not have worked, for each query. \n",
    "\n",
    "2. Do you notice anything different about the costs to those discussed in lecture? \n",
    "\n",
    "3. Please fill out the table below with the costs from running the Levenshtein algorithm discussed in lecture and the cost from using the edit_distance function we provided above. Copy-and-paste this markdown table into the cell below for your answers:\n",
    "```\n",
    "| Operation    | Cost (Lecture)|  Cost (edit_distance)  | Example\n",
    "| :----------: |:------------- | :-------------------------- | --------\n",
    "| Addition     | YOUR ANSWER   | YOUR ANSWER                 | \"aa\" -> \"aab\"\n",
    "| Deletion     | YOUR ANSWER   | YOUR ANSWER                 | \"aa\" -> \"a\"\n",
    "| None         | YOUR ANSWER   | YOUR ANSWER                 | \"aa\" -> \"aa\"\n",
    "| Substitution | YOUR ANSWER   | YOUR ANSWER                 | \"aa\" -> \"ab\"\n",
    "| Multiple     | YOUR ANSWER   | YOUR ANSWER                 | \"trawl\" --> \"claw\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3558b05fc430425c4f3710c9d0698b6a",
     "grade": true,
     "grade_id": "edit_distance_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The first query worked well as it found the exact match. As it found \"It's not a bunch of teenagers running around.\" we could see that it works well if there are small changes from the target. \n",
    "\n",
    "For the second query, it worked because it found the extremely similar result. The query seems to be the rephrase of what Simon said. Because the query's content and words were quite unique, we could see that it did not find similar matches for others in the list. \n",
    "\n",
    "The third query did not work despite it found 2 resemble ones. It is because the query is too short, it found results that have match in non-important words like \"You.\"\n",
    "\n",
    "The last query also did not work well because it omitted the \"maryland,\" which seems to be the most important word. Same as the third query, it did not work well because the query is very short. \n",
    "\n",
    "I realized that the cost for substitution is different to those we used in the lecture. Whereas we used cost of 2 for substitution in the lecture, the cost is 1 this time. Because of change in cost of substitution, the cost of multiple also changed. As the substitution cost decreased, cost for multiple also decreased.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "| Operation    | Cost (Lecture)|  Cost (edit_distance)  | Example\n",
    "| :----------: |:------------- | :-------------------------- | --------\n",
    "| Addition     | 1             | 1                           | \"aa\" -> \"aab\"\n",
    "| Deletion     | 1             | 1                           | \"aa\" -> \"a\"\n",
    "| None         | 0             | 0                           | \"aa\" -> \"aa\"\n",
    "| Substitution | 2             | 1                           | \"aa\" -> \"ab\"\n",
    "| Multiple     | 5             | 3                           | \"trawl\" --> \"claw\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2a: Changing the costs (Code Completion)<a class=\"anchor\" id=\"q2a\"></a>\n",
    "\n",
    "The above implementation has been using standard costs as we did in class. Sometimes we might want to alter the edit-distance penalty depending on the type of change we are making. In practice, the use of a simple uniform cost for all changes is suboptimal, since certain changes are more likely to happen than others. For instance, it is easier to mix up characters that are adjacent to each other on the keyboard (typos), and we'd like you to **reduce the cost of such substitutions of adjacent keyboard letters to 1.5 instead of 2**. You should use the provided list of adjacent keyboard letters, found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we provide you with a list of adjacent characters\n",
    "adj_chars = [('a', 'q'), ('a', 's'), ('a', 'z'), ('b', 'g'), ('b', 'm'), ('b', 'n'), ('b', 'v'), ('c', 'd'),\n",
    "             ('c', 'v'), ('c', 'x'), ('d', 'c'), ('d', 'e'), ('d', 'f'), ('d', 's'), ('e', 'd'), ('e', 'r'),\n",
    "             ('e', 'w'), ('f', 'd'), ('f', 'g'), ('f', 'r'), ('f', 'v'), ('g', 'b'), ('g', 'f'), ('g', 'h'),\n",
    "             ('g', 't'), ('h', 'g'), ('h', 'j'), ('h', 'm'), ('h', 'n'), ('h', 'y'), ('i', 'k'), ('i', 'o'),\n",
    "             ('i', 'u'), ('j', 'h'), ('j', 'k'), ('j', 'u'), ('k', 'i'), ('k', 'j'), ('k', 'l'), ('l', 'k'),\n",
    "             ('l', 'o'), ('m', 'b'), ('m', 'h'), ('n', 'b'), ('n', 'h'), ('o', 'i'), ('o', 'l'), ('o', 'p'),\n",
    "             ('p', 'o'), ('q', 'a'), ('q', 'w'), ('r', 'e'), ('r', 'f'), ('r', 't'), ('s', 'a'), ('s', 'd'),\n",
    "             ('s', 'w'), ('s', 'x'), ('t', 'g'), ('t', 'r'), ('t', 'y'), ('u', 'i'), ('u', 'j'), ('u', 'y'), \n",
    "             ('v', 'b'), ('v', 'c'), ('v', 'f'), ('w', 'e'), ('w', 'q'), ('w', 's'), ('x', 'c'), ('x', 's'), \n",
    "             ('x', 'z'), ('y', 'h'), ('y', 't'), ('y', 'u'), ('z', 'a'), ('z', 'x')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fae93c7c69c2296013418800d380259",
     "grade": false,
     "grade_id": "substitution_cost",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def substitution_cost_adj(query, message, i, j):\n",
    "    \"\"\"\n",
    "    Custom substitution cost:\n",
    "    The cost is 1.5 when substituting a pair of characters that can be found in adj_chars\n",
    "    Otherwise, the cost is 2. (Not 1 as it was before!)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    if query[i-1] == message[j-1]:\n",
    "        return 0 \n",
    "    elif (query[i-1],message[j-1]) in adj_chars:\n",
    "        return 1.5 \n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a96928b1237eaf9f4e5ecf25b8faa11",
     "grade": true,
     "grade_id": "substitution_cost_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert edit_distance(\"Levenshtein\",\"Levenstein\", insertion_cost, deletion_cost, substitution_cost_adj) == 1\n",
    "assert edit_distance(\"Levenshtein\",\"Lebenshtein\", insertion_cost, deletion_cost, substitution_cost_adj) == 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code below to reorder the top ten closest matches for each query using our new custom edit-distance discounts. Once again, this may take a few minutes to run.\n",
    "\n",
    "Note: It is ok to obtain duplicate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3450324d98a201385ba48f8b08cecb31",
     "grade": false,
     "grade_id": "cell-1e728ba1062ffed4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################\n",
      "It's like a bunch of people running around talking about nothing.\n",
      "#################################################################\n",
      "[0.00] BRUCE: It's like a bunch of people running around talking about nothing.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[39.50] KRIS: It's not a bunch of teenagers running around.\n",
      "\t(Keeping Up With the Kardashians - Kris ``The Cougar'' Jenner)\n",
      "[42.00] KHLOE: It's like, what are you talking about?\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[45.00] KOURTNEY: It's like an explosion in your pantyhose.\n",
      "\t(Keeping Up With the Kardashians - I'd Rather Go Naked... Or Shopping)\n",
      "[47.00] KHLOE: It's like I want to learn how to do that.\n",
      "\t(Keeping Up With the Kardashians - Distance Makes the Heart Grow Fonder)\n",
      "[48.00] BRUCE: That's why you're running around wearing black.\n",
      "\t(Keeping Up With the Kardashians - I'd Rather Go Naked... Or Shopping)\n",
      "[48.00] BRUCE: That's why you're running around wearing black.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[48.50] KIM: It's like it has separation anxiety or something.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[50.00] ROB: I have a bunch of connections in the industry.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[50.00] ROB: I have a bunch of connections in the industry.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "\n",
      "#############################################################################################\n",
      "Never say to a famous person that this possible endorsment would bring 'er to the spot light.\n",
      "#############################################################################################\n",
      "[57.00] SIMON: You don't tell an international celebrity that this possible endorsement could bring her back into the spotlight.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[75.50] KIM: I didn't want to go on this trip and I'm not talking to them at all.\n",
      "\t(Keeping Up With the Kardashians - Kardashian Civil War)\n",
      "[76.00] ELIZABETH: I reminded you this morning that you needed to bring the designs.\n",
      "\t(Keeping Up With the Kardashians - Family vs. Money)\n",
      "[76.50] ALEX: I feel like people want to see you, like, back in the spotlight.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[76.50] BRUCE: Everything's going to be fine.\n",
      "\t(Keeping Up With the Kardashians - The Missing Ring)\n",
      "[77.00] KHLOE: Trust me, this is the most entertaining part of the entire night.\n",
      "\t(Keeping Up With the Kardashians - Birthday Suit)\n",
      "[79.50] KHLOE: Your secret is safe with me.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[80.50] BRUCE: You know, I've seen her do this before, and it's just not right.\n",
      "\t(Keeping Up With the Kardashians - Baby Blues)\n",
      "[80.50] KHLOE: You are walking me me down...\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[80.50] KHLOE: You are walking me me down...\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "\n",
      "####################################\n",
      "Your yapping is making my head ache!\n",
      "####################################\n",
      "[20.00] KIM: Your yappity voice is giving me a headache!\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[20.00] KIM: Your yappity voice is giving me a headache!\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[26.00] KIM: You want to make peace?\n",
      "\t(Keeping Up With the Kardashians - Family vs. Money)\n",
      "[27.00] KRIS: You're going to make a scene.\n",
      "\t(Keeping Up With the Kardashians - What's Yours Is Mine)\n",
      "[30.00] BRUCE: You're diving in for the cash?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[31.00] KHLOE: You are walking me me down...\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[31.00] KHLOE: You are walking me me down...\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[32.50] BRUCE: You're diving in for   the  cash?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[33.50] KHLOE: Your secret is safe with me.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[34.50] ROB: I'm going to tell her.\n",
      "\t(Keeping Up With the Kardashians - Meet the Kardashians)\n",
      "\n",
      "######################################\n",
      "I'm going to Maryland, did I tell you?\n",
      "######################################\n",
      "[22.00] LAMAR: I'm going to need it.\n",
      "\t(Keeping Up With the Kardashians - I Want Your Sex)\n",
      "[23.50] ROB: I'm going to tell her.\n",
      "\t(Keeping Up With the Kardashians - Meet the Kardashians)\n",
      "[26.50] KHLOE: I'm going to move to New York.\n",
      "\t(Keeping Up With the Kardashians - Distance Makes the Heart Grow Fonder)\n",
      "[26.50] KHLOE: I'm going to move to New York.\n",
      "\t(Keeping Up With the Kardashians - Free Khloe)\n",
      "[27.00] SCOTT: I'm going to have six or seven?\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[27.00] SCOTT: I'm going to have six or seven?\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[27.50] KRIS: I'm going to lunch with Lisa.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[27.50] FRIEND: I'm going to get canned inside.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[27.50] FRIEND: I'm going to get canned inside.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[28.00] LAMAR: I'm going to go play with Rob.\n",
      "\t(Keeping Up With the Kardashians - Blind Date)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(\"#\" * len(query))\n",
    "    print(query)\n",
    "    print(\"#\" * len(query))\n",
    "\n",
    "    # we compare the query with top_10 to save time because\n",
    "    # we already know that top_10 will be close to the query\n",
    "    # so there is no need to check all flat_msgs\n",
    "    for score, msg in edit_distance_search(query, top_10, insertion_cost, deletion_cost, substitution_cost_adj)[:10]:\n",
    "        print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
    "            score,\n",
    "            msg['speaker'],\n",
    "            msg['text'],\n",
    "            msg['episode_title']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2b: Query Discussion and Analysis (Free Response)<a class=\"anchor\" id=\"q2b\"></a>\n",
    "\n",
    "4. How do the results after cost changes different compare to that of default `edit_distance` metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b05a6884e5a47e9f01bfbd45cd9f459e",
     "grade": true,
     "grade_id": "substitution_cost_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "For the first query, there are some results that have been influenced by the factor of adjacnet keyboard letters. Whereas \"[37.00] KIM: It's like it has separation anxiety or something.\" was ranked 4th place in the default metric, we can see that its rank was pushed back to 8th place. Also, there was a tie between KHOLE and KOURTNEY's script with the cost of [37.00] in the default metric. After the manipulation, we can see that KHLOE's now cost more, probably assuming that words were not adjacent compared to KOURTNEY's. \n",
    "\n",
    "For the second query, it is interesting that [59.00] KRIS does not even appear from the result after the cost change. I could infer that KRIS's words were not adjacent as the others and affected a lot. \n",
    "\n",
    "For the third query, we can see that the ties between BRUCE and KHOLE has been ranked. Bruce's line were highly affected by the adjacency so that it does not appear after manipulation. For the ties with BRUCE and KHLOE with the cost of [22.00], we can see they are now also ranked. Also, a new script line of [34.50] ROB: appeared on a list as it probably had lesser cost with adjacent words. \n",
    "\n",
    "Lastly, we initially had 5 ties with cost of [19.00] in a default metric. Now, they all have different values exept the duplication, with the cost of [26.5], [27.00] and [27.5] after the modification by the adjacency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "<p style='text-align:right; font-size:11px'>[[contents](#contents)]</p>\n",
    "\n",
    "## Cosine Similarity<a class=\"anchor\" id=\"part2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b78e675c6ab81fb450c01f4569c1cf9",
     "grade": false,
     "grade_id": "cell-30f8447c4e8aec09",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### A high-level overview\n",
    "\n",
    "Our overall goal of this assignment is to build a system where we can compute the text similarity between queries and our datasets quickly. From the previous part, we tried using variations of minimum edit distance as the metric. Now, we will try another method -- comparing two text through cosine similarity score. To accomplish queries and compute cosine similarities, as we learned in class, we will need to represent documents as ***vectors***! A common method of representing documents as vectors is by using \"term frequency-inverse document frequency\" (TF-IDF) scores. (Check the lecture modules on Canvas if you need a refresher). The notation here is consistent with the in-class hand out, so if you haven't read over it -- you should!\n",
    "\n",
    "Consider the TF-IDF representation of a document and a query: $\\vec{d_j}$ and $\\vec{q}$, respectively. Elements of these vectors are very often zero because the term frequency of most words in most documents is zero. Stated differently, most words don't appear in most documents! Consider a query that has 5 words in it and a vocabulary that has 20K words in it -- only .025% of the elements of the vector representation of the query are nonzero! When a vector (or a matrix) has very few nonzero entries, it is called \"sparse.\" We can take advantage of the sparsity of tf-idf document representations to compute cosine similarity quickly. We will first build some data stuctures that allow for faster querying of statistics, and then we will build a function that quickly computes cosine similarity between queries and documents.\n",
    "\n",
    "### A starting point\n",
    "We will use an **inverted index** for efficiency. This is a sparse term-centered representation that allows us to quickly find all documents that contain a given term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Write a function to construct the inverted index (Code Completion)<a class=\"anchor\" id=\"q3\"></a>\n",
    "\n",
    "As in class, the inverted index is a key-value structure where the keys are terms and the values are lists of *postings*. In this case, we record the documents a term occurs in as well as the **count** of that term in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bda30fd8612c4f99c56924db18e956cb",
     "grade": false,
     "grade_id": "build_inverted_index",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_inverted_index(msgs):\n",
    "    \"\"\" Builds an inverted index from the messages.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    msgs: list of dicts.\n",
    "        Each message in this list already has a 'toks'\n",
    "        field that contains the tokenized message.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    inverted_index: dict\n",
    "        For each term, the index contains \n",
    "        a sorted list of tuples (doc_id, count_of_term_in_doc)\n",
    "        such that tuples with smaller doc_ids appear first:\n",
    "        inverted_index[term] = [(d1, tf1), (d2, tf2), ...]\n",
    "        \n",
    "    Example\n",
    "    =======\n",
    "    \n",
    "    >> test_idx = build_inverted_index([\n",
    "    ...    {'toks': ['to', 'be', 'or', 'not', 'to', 'be']},\n",
    "    ...    {'toks': ['do', 'be', 'do', 'be', 'do']}])\n",
    "    \n",
    "    >> test_idx['be']\n",
    "    [(0, 2), (1, 2)]\n",
    "    \n",
    "    >> test_idx['not']\n",
    "    [(0, 1)]\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    inverted_index = {} # key: toks, value: list of tokened msgs \n",
    "        \n",
    "    for doc_id, msg in enumerate(msgs):\n",
    "        \n",
    "        listofTerms = msg['toks'] #list of tokened msgs \n",
    "        countTerms = Counter(listofTerms)\n",
    "                \n",
    "        for k,v in countTerms.items():\n",
    "            if k in inverted_index.keys():\n",
    "                inverted_index[k].append((doc_id,v))\n",
    "            else:\n",
    "                inverted_index[k] = [(doc_id, v)]\n",
    "                \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31126aff0fb57f211e2e1e16981abe86",
     "grade": true,
     "grade_id": "build_inverted_index_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "inv_idx = build_inverted_index(flat_msgs)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(inv_idx) <= 10000 \n",
    "assert [i[0] for i in inv_idx['bruce']] == sorted([i[0] for i in inv_idx['bruce']])\n",
    "assert len(inv_idx['bruce']) < len(inv_idx['kim'])\n",
    "assert len(inv_idx['bruce']) >= 400 and len(inv_idx['bruce']) <= 435\n",
    "assert len(inv_idx['baby']) >= 250 and len(inv_idx['baby']) <= 300\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b78272072ed8afbd520c794dacef5132",
     "grade": false,
     "grade_id": "cell-c92587934d16cc87",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q4a Using the inverted index for boolean search (Code Completion)<a class=\"anchor\" id=\"q4a\"></a>\n",
    "\n",
    "In this section we will use the inverted index you constructed to perform an efficient boolean search. The boolean model was one of the early information retrieval models, and continues to be used in applications today.\n",
    "\n",
    "A boolean search works by searching for documents which match the boolean expression of the query. Three main operators in a boolean search are `AND` `OR` and `NOT`. For example, the query `\"Ned\" AND \"Rob\"` would return any document which contains both the words \"Ned\" and \"Rob\".\n",
    "\n",
    "Here, we will treat a query as a simple two-word search with exclusion---that is, we will implement a search with `NOT` instead of `AND`. For example, the query words \"kardashian\", \"kim\" would be equivalent to the boolean expression `\"kardashian\" NOT \"kim\"`, and will return any document which contains the word \"kardashian\" while not containing the word \"kim\".\n",
    "\n",
    "#### In class we implemented the Merge Postings Algorithm, review the code [here](https://colab.research.google.com/drive/1Egg26OyynPuWizjEhSzMJ4rad9TwFQ1s?usp=sharing).\n",
    "Reminder: the algorithm takes two postings lists (A and B), then proceeds as follows:\n",
    "\n",
    "------------------------------------------\n",
    "Initialize empty list (called merged list M)\n",
    "\n",
    "Start: Pointer at the first element of both A and B\n",
    "\n",
    "Do: Does it point to the same document ID in each list?\n",
    "\n",
    "    Yes: Append document ID to M, advance pointer in both A and B\n",
    "    No: Advance (to the right) the pointer with the smaller ID\n",
    "    \n",
    "End: When we attempt to advance a pointer already at the end of its list\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "The above Merge Postings Algorithm can be thought of a boolean search with the `AND` operator. Write a function `boolean_search` that implements a similar algorithm with the `NOT` operator using the inverted index.\n",
    "\n",
    "**Note:** Make sure you convert the `query_word` and `not_word` to lowercase. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f72c771deef339673634f82c98d2148",
     "grade": false,
     "grade_id": "boolean_search",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def boolean_search(query_word,not_word, inverted_index):\n",
    "    \"\"\" Search the collection of documents for the given query_word \n",
    "        provided that the documents do not include the not_word\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query_word: string,\n",
    "        The word we are searching for in our documents.\n",
    "    \n",
    "    not_word: string,\n",
    "        The word excluded from our documents.\n",
    "    \n",
    "    index: an inverted index as above\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    results: list of ints\n",
    "        Sorted List of results (in increasing order) such that every element is a `doc_id`\n",
    "        that points to a document that satisfies the boolean\n",
    "        expression of the query.\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    returnList = [] #list of ints \n",
    "\n",
    "    query = query_word.lower()\n",
    "    q = sorted(inverted_index[query])\n",
    "\n",
    "    excluded = not_word.lower()\n",
    "    e = sorted(inverted_index[excluded])\n",
    "    \n",
    "    i=0 \n",
    "    j=0\n",
    "    \n",
    "    while i<len(q) and j<len(e):\n",
    "        \n",
    "        #don't appened, advance both pointer \n",
    "        if q[i][0]==e[j][0]: \n",
    "            i+=1\n",
    "            j+=1 \n",
    "        \n",
    "        #advance smaller ID\n",
    "        elif q[i][0]<e[j][0]: \n",
    "            returnList.append(q[i][0])\n",
    "            i+=1 \n",
    "        \n",
    "        else:\n",
    "            j+=1 \n",
    "        \n",
    "    if i<len(q):\n",
    "        for r in range(i,len(q)):\n",
    "            returnList.append(q[r][0])\n",
    "    \n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1882633e5ee3bd65381e16cf786b8d04",
     "grade": true,
     "grade_id": "boolean_search_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "result0_start_time = time.time()\n",
    "result0 = boolean_search('ice','cream', inv_idx)\n",
    "result0_execution_time = time.time() - result0_start_time\n",
    "result3 = boolean_search('puppy','dog', inv_idx)\n",
    "result1= boolean_search('Kardashian','Kim',inv_idx)\n",
    "result4= boolean_search('cake','cake',inv_idx)\n",
    "assert result0_execution_time < 1.0\n",
    "assert type(result1) == list\n",
    "assert len(result3) == 7\n",
    "assert len(result4)==0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd3205caee59f446a33b07d32135cfeb",
     "grade": false,
     "grade_id": "cell-f8a80e02cfb14fc4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q4b Using the inverted index for boolean search (Free Response)<a class=\"anchor\" id=\"q4b\"></a>\n",
    "\n",
    "Earlier in the assignment, we already explored search techniques (edit distance) which are able to find a wider variety of relevant results. Why might you want to use a boolean search with an inverted index instead? Justify your answer by explaining the difference between edit distance and boolean search with inverted index, *and* including at least one example where boolean search with inverted index would be a better choice than an edit distance search. **You will lose points if you do not provide an example!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6698b5447fbcba245c0b701a865ee70",
     "grade": false,
     "grade_id": "cell-7975337e0b58caa8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c388d41a9422fbbd92aeab9dd7278fc7",
     "grade": true,
     "grade_id": "boolean_search_ans",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "Using inverted index is more efficient because it allows to quickly find all documents that contain a given term as it is term-based. As described, most of words do not appear in most of the documents. If we use the edit distance, we would have to go through all document and calculate the edit distance, which is time consuming. In contrast, when we use inverted index, we only look at the documents that contain the word, so it is more efficient. For instance, let's say there is a \"hello\" query, which appears only in 5 documents out of 1,000 documents in total.  If we use edit distance, we would go over all 1,000 documents and find out there is only 5 documents that contains \"hello.\" This will take huge amount of time. However, when we use the boolean search with inverted index, we would start off with the 5 documents that contains \"hello\". Likewise, using boolean search with inverted index would be the better choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36617fb84a0ff5ca37d43a9754ce5e2d",
     "grade": false,
     "grade_id": "cell-1ad52f18c9e1026b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-top: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold; text-align: center;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac7987714cf59e1a3ab7b8193c078fbf",
     "grade": false,
     "grade_id": "cell-1d10f0b8dc4ffc19",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q5 Compute IDF *using* the inverted index (Code Completion)<a class=\"anchor\" id=\"q5\"></a>\n",
    "\n",
    "Write a function `compute_idf` that uses the inverted index to efficiently compute IDF values.\n",
    "\n",
    "Words that occur in a very small number of documents are not useful in many cases, so we ignore them. Use a parameter `min_df`\n",
    "to ignore all terms that occur in strictly fewer than `min_df=10` documents.\n",
    "\n",
    "Similarly, words that occur in a large *fraction* of the documents don't bring any more information for some tasks. Use a parameter `max_df_ratio` to trim out such words. For example, `max_df_ratio=0.95` means ignore all words that occur in more than 95% of the documents.\n",
    "\n",
    "As a reminder, we define the IDF statistic as...\n",
    "$$ IDF(t) = \\log \\left(\\frac{N}{1 + DF(t)} \\right) $$\n",
    "\n",
    "where $N$ is the total number of docs and $DF(t)$ is the number of docs containing $t$. Keep in mind, there are other definitions of IDF out there, so if you go looking for resources on the internet, you might find differing (but also valid) accounts. In practice the base of the log doesn't really matter, however you should use base 2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab6828ded82a44cdac7c436e89e39cb9",
     "grade": false,
     "grade_id": "compute_idf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=10, max_df_ratio=0.95):\n",
    "    \"\"\" Compute term IDF values from the inverted index.\n",
    "    Words that are too frequent or too infrequent get pruned.\n",
    "    \n",
    "    Hint: Make sure to use log base 2.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    inv_idx: an inverted index as above\n",
    "    \n",
    "    n_docs: int,\n",
    "        The number of documents.\n",
    "        \n",
    "    min_df: int,\n",
    "        Minimum number of documents a term must occur in.\n",
    "        Less frequent words get ignored. \n",
    "        Documents that appear min_df number of times should be included.\n",
    "    \n",
    "    max_df_ratio: float,\n",
    "        Maximum ratio of documents a term can occur in.\n",
    "        More frequent words get ignored.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    idf: dict\n",
    "        For each term, the dict contains the idf value.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    idf = dict()\n",
    "    \n",
    "    #iterate through all query terms\n",
    "    for i in inv_idx:\n",
    "        \n",
    "        count = len(inv_idx[i])\n",
    "        \n",
    "        if (count < min_df or  count/n_docs > max_df_ratio):\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            result = math.log2(n_docs/(1+len(inv_idx[i])))\n",
    "            idf[i] = round(result,2)\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77bab12b5ea13930f9ec1a91a3573609",
     "grade": true,
     "grade_id": "compute_idf_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "idf_dict = compute_idf(inv_idx, len(flat_msgs))\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(idf_dict) < len(inv_idx)\n",
    "assert 'blah' not in idf_dict\n",
    "assert 'blah' in inv_idx \n",
    "assert '.' in idf_dict\n",
    "assert '3' not in idf_dict\n",
    "assert idf_dict['bruce'] >= 6.0 and idf_dict['bruce'] <= 7.0\n",
    "assert idf_dict['baby'] >= 6.0 and idf_dict['baby'] <= 8.0\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00c5bdb39bb8a9fc342acc5bacc0ad4b",
     "grade": false,
     "grade_id": "cell-976a9144b11db274",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q6 Compute the norm of each document using the inverted index (Code Completion)<a class=\"anchor\" id=\"q6\"></a>\n",
    "\n",
    "Recalling our tf-idf vector representation of documents, we can compute the \"norm\" as the norm (length) of the vector representation of that document. More specifically, the norm of a document $j$, denoted as $\\left|\\left| d_j \\right|\\right|$, is given as follows...\n",
    "\n",
    "$$ \\left|\\left| d_j \\right|\\right| = \\sqrt{\\sum_{\\text{word } i} (tf_{ij} \\cdot idf_i)^2} $$\n",
    "\n",
    "Once again, this can be quite efficiently implemented using the inverted index. Below, write a function that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "881f76fc025a8f6d4c5c516471c553db",
     "grade": false,
     "grade_id": "compute_doc_norms",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\" Precompute the euclidean norm of each document.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    index: the inverted index as above\n",
    "    \n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "    \n",
    "    n_docs: int,\n",
    "        The total number of documents.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    norms: np.array, size: n_docs\n",
    "        norms[i] = the norm of document i.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "     \n",
    "    norms = np.zeros(n_docs)\n",
    "            \n",
    "    for k,v in idf.items():\n",
    "        for docId, count in index[k]:\n",
    "            norms[docId] += (count*idf[k])**2\n",
    "        \n",
    "    return np.sqrt(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69f8f675f4a3f8a07b202bbb8c0bbf7e",
     "grade": true,
     "grade_id": "compute_doc_norms_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "doc_norms = compute_doc_norms(inv_idx, idf_dict, len(flat_msgs))\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(flat_msgs) == len(doc_norms)\n",
    "assert doc_norms[3722] == 0\n",
    "assert max(doc_norms) < 80\n",
    "assert doc_norms[1] >= 15.5 and doc_norms[1] <= 17.5\n",
    "assert doc_norms[5] >= 6.5 and doc_norms[5] <= 8.5\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Efficiently compute the dot product *using* the inverted index (Code Completion)<a class=\"anchor\" id=\"q7\"></a>\n",
    "\n",
    "We are getting closer to being able to use our inverted index to do a fast implementation of cosine similarity. There is one more ingredient that will come in handy when it comes to optimization. Recall the definition of cosine similarity (where ${q}$ is a query tf-idf vector and ${d_j}$ is the tf-idf vector of the ${j}$th document):\n",
    "\n",
    "$$ cossim(\\vec{q}, \\vec{d_j}) = \\frac{\\vec{q} \\cdot \\vec{d_j}}{\\|\\vec{q}\\| \\cdot \\|\\vec{d_j}\\|}$$\n",
    "\n",
    "Specifically, let's focus on the numerator, which computes a dot product. If we recall the definition of the dot product, this can be written as:\n",
    "\n",
    "$$ \\vec{q} \\cdot \\vec{d_j} = \\sum_{i} {q_i} * {d_i}_j $$\n",
    "\n",
    "Here ${q_i}$ and ${d_i}_j$ are the $i$-th dimension of the vectors $q$ and ${d_j}$ respectively.\n",
    "Because many ${q_i}$ and ${d_i}_j$ are zero, it is actually a bit wasteful to actually create the vectors $q$ and $d_j$ as numpy arrays; this is the method that you saw in class.\n",
    "\n",
    "A faster approach to computing the numerator term (dot product) for cosine similarity involves quickly computing the above summation using the inverted index. Recall from class that this is achieved via a *term-at-a-time* approach, iterating over ${q}_j$ that are nonzero (i.e. ${q}_j$ such that the word $j$ appears in the query) and building up *score accumulators* for each document as you iterate.\n",
    "\n",
    "In this question, you will write a function that implements this term-at-a-time score accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47be2c941fec8678a3828b3282cbc32a",
     "grade": false,
     "grade_id": "compute_scores",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accumulate_dot_scores(query_word_counts, index, idf):\n",
    "    \"\"\" Perform a term-at-a-time iteration to efficiently compute the numerator term of cosine similarity across multiple documents.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query_word_counts: dict,\n",
    "        A dictionary containing all words that appear in the query;\n",
    "        Each word is mapped to a count of how many times it appears in the query.\n",
    "        In other words, query_word_counts[w] = the term frequency of w in the query.\n",
    "        You may safely assume all words in the dict have been already lowercased.\n",
    "    \n",
    "    index: the inverted index as above,\n",
    "    \n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    doc_scores: dict\n",
    "        Dictionary mapping from doc ID to the final accumulated score for that doc\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    doc_scores = {} \n",
    "\n",
    "    for k,v in query_word_counts.items():\n",
    "        \n",
    "        for docId, count in index[k]:\n",
    "            \n",
    "            if docId not in doc_scores: \n",
    "                doc_scores[docId] = 0\n",
    "            doc_scores[docId] += idf[k] * count * v * idf[k]\n",
    "             \n",
    "    return doc_scores\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c92785cf5d75e9cb0db0d004b4f3ff12",
     "grade": true,
     "grade_id": "compute_scores_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "example_query_words = {\"like\": 2, \"mother\": 1, \"daughter\": 1}\n",
    "start_time = time.time()\n",
    "scores = accumulate_dot_scores(example_query_words, inv_idx, idf_dict)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(scores) > 3000\n",
    "assert max(scores.values()) < 220\n",
    "assert scores[13] > 27 and scores[13] < 30\n",
    "assert scores[324] > 55 and scores[324] < 58\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a87bfdab83142a40ccfae4361da1e46c",
     "grade": false,
     "grade_id": "cell-7fef1edededcee58",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q8a Find the most similar messages to the quotes (Code Completion)<a class=\"anchor\" id=\"q8\"></a>\n",
    "\n",
    "The goal of this section is to implement `index_search`, a fast implementation of cosine similarity. You will then test your answer by running the search function using the code provided. Briefly discuss why it worked, or why it might not have worked, for each query.\n",
    "\n",
    "The goal of `index_search` is to compute the cosine similarity between the query and each document in the dataset. Naively, this computation requires you to compute dot products between the query tf-idf vector $q$ and each document's tf-idf vector $d_i$, which sounds very computationally expensive!\n",
    "\n",
    "Thankfully, you should be able to use the sparsity of the tf-idf representation and the data structures you created to your advantage. We have already laid down all the groundwork we need for this in the previous questions---all that is left to do here is to put it all together!\n",
    "\n",
    "Once again, recall that our goal is to compute the cosine similarity:\n",
    "\n",
    "$$ cossim(\\vec{q}, \\vec{d_j}) = \\frac{\\vec{q} \\cdot \\vec{d_j}}{\\|\\vec{q}\\| \\cdot \\|\\vec{d_j}\\|}$$\n",
    "\n",
    "Just to recap, thus far we have created the following:\n",
    "- An inverted index mapping from word to the documents they appear in\n",
    "- IDF values for every word that does not appear too rarely or too frequently\n",
    "- Vector norms for the tf-idf vectors of every document\n",
    "- A function that uses term-at-a-time iteration to efficiently compute dot products between the query and multiple documents (that is, the numerator of the cosine similarity)\n",
    "\n",
    "Your task in this function will be to assemble the above components to compute the cosine similarities between the query and documents, and use these to rank the documents.\n",
    "\n",
    "**Note:** Convert the query to lowercase, and use the `nltk.tokenize.TreebankWordTokenizer` to tokenize the query (provided to you as the `tokenizer` parameter). The transcripts have already been tokenized this way. <br>\n",
    "\n",
    "**Note 2:** For `index_search`, you need not remove punctuation tokens from the tokenized query before searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "974bf808c87951993d80236ae03fdbc0",
     "grade": false,
     "grade_id": "cell-b39f99a59a119d67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Aside:** Precomputation\n",
    "\n",
    "In many settings, we will need to repeat the same kind of operation many times. Often, part of the input doesn't change.\n",
    "Queries against the Kardashians transcript are like this: we want to run more queries (in the real world we'd want to run a lot of them every second, even) but the data we are searching doesn't change.\n",
    "\n",
    "We could write an `index_search` function which taking the `query` and the `msgs` as input, and the function would look like:\n",
    "\n",
    "    def index_search(query, msgs):\n",
    "        inv_idx = build_inverted_index(msgs)\n",
    "        idf = compute_idf(inv_idx, len(msgs))\n",
    "        doc_norms = compute_doc_norms(inv_idx)\n",
    "        # do actual search\n",
    "\n",
    "\n",
    "But notice that the first three lines only depend on the messages. Imagine if we run this a million times with different queries but the same collection of documents: we'd wastefully recompute the index, the IDFs and the norms every time and discard them. It's a better idea, then, to precompute them just once, and pass them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94f16111f94539105aa62a3cc75c7907",
     "grade": false,
     "grade_id": "cell-9f5e36e0536c8e91",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "inv_idx = build_inverted_index(flat_msgs)\n",
    "\n",
    "idf = compute_idf(inv_idx, len(flat_msgs),\n",
    "                  min_df=10,\n",
    "                  max_df_ratio=0.1)  # documents are very short so we can use a small value here\n",
    "                                     # examine the actual DF values of common words like \"the\"\n",
    "                                     # to set these values\n",
    "\n",
    "inv_idx = {key: val for key, val in inv_idx.items()\n",
    "           if key in idf}            # prune the terms left out by idf\n",
    "\n",
    "doc_norms = compute_doc_norms(inv_idx, idf, len(flat_msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79c677982762ca74427d9deb0a5d7934",
     "grade": false,
     "grade_id": "index_search",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def index_search(query, index, idf, doc_norms, score_func=accumulate_dot_scores, tokenizer=treebank_tokenizer):\n",
    "    \"\"\" Search the collection of documents for the given query\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query: string,\n",
    "        The query we are looking for.\n",
    "    \n",
    "    index: an inverted index as above\n",
    "    \n",
    "    idf: idf values precomputed as above\n",
    "    \n",
    "    doc_norms: document norms as computed above\n",
    "    \n",
    "    score_func: function,\n",
    "        A function that computes the numerator term of cosine similarity (the dot product) for all documents.\n",
    "        Takes as input a dictionary of query word counts, the inverted index, and precomputed idf values.\n",
    "        (See Q7)\n",
    "    \n",
    "    tokenizer: a TreebankWordTokenizer\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    results, list of tuples (score, doc_id)\n",
    "        Sorted list of results such that the first element has\n",
    "        the highest score, and `doc_id` points to the document\n",
    "        with the highest score.\n",
    "    \n",
    "    Note: \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    results = []\n",
    "    tokenizedQuery = tokenizer.tokenize(query.lower())\n",
    "    \n",
    "    #1. calculate term frequency of query \n",
    "    \n",
    "    tfQuery = {}\n",
    "    qNorm = 0\n",
    "    \n",
    "    for query in tokenizedQuery:\n",
    "        tfQuery[query] = tfQuery.get(query, 0)+1   \n",
    "        \n",
    "        #2. calculate the norm \n",
    "        if query in idf.keys():\n",
    "            qNorm +=np.square(tfQuery[query]*idf[query])\n",
    "            \n",
    "    qNorm = np.sqrt(qNorm)\n",
    "    \n",
    "    \n",
    "    #3 calculate\n",
    "    \n",
    "    coSim = {}\n",
    "    \n",
    "    for query in tokenizedQuery:\n",
    "        if query in index:\n",
    "            for doc_id, count in index[query]:\n",
    "                num = idf[query] * count * tfQuery[query] * idf[query]\n",
    "                denom = qNorm * doc_norms[doc_id]\n",
    "                coSim[doc_id] = coSim.get(doc_id, 0) + (num/denom)\n",
    "        \n",
    "    for doc_id, score in coSim.items():\n",
    "        results.append((score, doc_id)) \n",
    "        \n",
    "    results = [x for x in sorted(results, key = lambda x: (-x[0],x[1]))]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76b2420b3375e90b666ca99972bd55df",
     "grade": true,
     "grade_id": "index_search_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################\n",
      "It's like a bunch of people running around talking about nothing.\n",
      "#################################################################\n",
      "[1.00] BRUCE: It's like a bunch of people running around talking about nothing.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[0.61] KRIS: It's not a bunch of teenagers running around.\n",
      "\t(Keeping Up With the Kardashians - Kris ``The Cougar'' Jenner)\n",
      "[0.46] ROB: I have a bunch of connections in the industry.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[0.46] ROB: I have a bunch of connections in the industry.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[0.43] KHLOE: She's, like, running.\n",
      "\t(Keeping Up With the Kardashians - Match Made in Hell)\n",
      "[0.42] ROB: She got me a bunch of interviews.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[0.42] ROB: She got me a bunch of interviews.\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[0.41] KHLOE: Not running.\n",
      "\t(Keeping Up With the Kardashians - The Missing Ring)\n",
      "[0.41] KOURTNEY: We don't want a bunch of on the counter.\n",
      "\t(Keeping Up With the Kardashians - All for One and One for Kim)\n",
      "[0.40] JOSIE: He cowrote a bunch of the big hits.\n",
      "\t(Keeping Up With the Kardashians - The Kardashians Take NYC)\n",
      "\n",
      "#############################################################################################\n",
      "Never say to a famous person that this possible endorsment would bring 'er to the spot light.\n",
      "#############################################################################################\n",
      "[0.44] ROB: With a light,crispy panko breading.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.44] ROB: With a light,crispy panko breading.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.44] ROB: With a light,crispy panko breading.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.41] KHLOE: You've never blasted outdual-LED light then sucked in everydetail in 8-Megapixels.\n",
      "\t(Keeping Up With the Kardashians - All for One and One for Kim)\n",
      "[0.40] SIMON: Make light of this and have dessert.\n",
      "\t(Keeping Up With the Kardashians - What's Yours Is Mine)\n",
      "[0.38] KIM: Bring a d ofash.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.38] KIM: Bring a d ofash.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.38] WOMAN: Bring it.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[0.38] KIM: Bring a d ofash.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.37] KOURTNEY: I say we load up thebÃ©bÃ©;I'll bring this and this.\n",
      "\t(Keeping Up With the Kardashians - Leaving the Nest)\n",
      "\n",
      "####################################\n",
      "Your yapping is making my head ache!\n",
      "####################################\n",
      "[0.69] SCOTT: The head!\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[0.52] DOCTOR: So here's the head.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.52] NURSE: His head is out.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[0.52] KHLOE: His head is out.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.52] NURSE: His head is out.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.50] KHLOE: I'm making a unicorn.\n",
      "\t(Keeping Up With the Kardashians - The Missing Ring)\n",
      "[0.50] LAMAR: I'm making a sandwich.\n",
      "\t(Keeping Up With the Kardashians - The Missing Ring)\n",
      "[0.48] KIM: Your whole head is gray.\n",
      "\t(Keeping Up With the Kardashians - My Bodyguard)\n",
      "[0.48] BRUCE: Don't let anything go to your head.\n",
      "\t(Keeping Up With the Kardashians - Kim Becomes a Diva)\n",
      "[0.47] KOURTNEY: And that is not where my head is at at all.\n",
      "\t(Keeping Up With the Kardashians - Kim Becomes a Diva)\n",
      "\n",
      "######################################\n",
      "I'm going to Maryland, did I tell you?\n",
      "######################################\n",
      "[1.00] BRUCE: Did I tell you I'm going to maryland?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.74] ROB: I'm going to tell her.\n",
      "\t(Keeping Up With the Kardashians - Meet the Kardashians)\n",
      "[0.70] ADRIENNE: What did I just tell you?\n",
      "\t(Keeping Up With the Kardashians - Kim Becomes a Diva)\n",
      "[0.60] BRUCE: Why didn't you tell me?\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[0.60] BRUCE: Why didn't you tell me?\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[0.58] BRUCE: What else did Stevens tell you?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.56] FRIEND: I'm going to .\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.56] FRIEND: I'm going to .\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.56] KOURTNEY: I'm going.\n",
      "\t(Keeping Up With the Kardashians - Kim Becomes a Diva)\n",
      "[0.56] KHLOE: I'm going to barf.\n",
      "\t(Keeping Up With the Kardashians - You Are So Pregnant Dude)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "results = index_search(queries[1], inv_idx, idf, doc_norms)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert type(results[0]) == tuple\n",
    "assert max(results)[0] == results[0][0]\n",
    "assert results[0][0] >= 0.4 and results[0][0] <= 0.48\n",
    "assert execution_time <= 1.0\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    print(\"#\" * len(query))\n",
    "    print(query)\n",
    "    print(\"#\" * len(query))\n",
    "\n",
    "    for score, msg_id in index_search(query, inv_idx, idf, doc_norms)[:10]:\n",
    "        print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
    "            score,\n",
    "            flat_msgs[msg_id]['speaker'],\n",
    "            flat_msgs[msg_id]['text'],\n",
    "            flat_msgs[msg_id]['episode_title'])) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d675100bcc544a64331aaf6830632cd4",
     "grade": false,
     "grade_id": "cell-f79bce72b4282bde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q8b Find the most similar messages to the quotes (Free Response)<a class=\"anchor\" id=\"q8b\"></a>\n",
    "\n",
    "Briefly discuss why cosine similarity worked, or why it might not have worked, **for each query**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c1e5d7e0052ed58c33c9fb5576ccc54",
     "grade": false,
     "grade_id": "cell-7b71a29cbc6a0190",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "174a55d92226ba5fa3e6bd98cf424b50",
     "grade": true,
     "grade_id": "index_search_ans",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "- For the first query, the cosine similarity worked because there is the exact match.\n",
    "\n",
    "\n",
    "- For the second query, it failed to work. For most of the result, it matched the word \"light.\" However, light is not the most important word, it should have search for the \"spotlight\" added together. I assume that the cossine similarity could not capture the spaces well.\n",
    "\n",
    "\n",
    "- For the third query, if failed to work. For most of the result, it matched the word \"head.\"I realized from the result of second and third one that cosine similarity focuses on a specific word rather than the overall similarity. \n",
    "\n",
    "\n",
    "- It worked for the last query. It found the almost exact match. I could infer that cossine similarity could capture regardless of the word placement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef5221b2f70c421bf0a9a34aa8cad8a0",
     "grade": false,
     "grade_id": "cell-40593bc03404aacd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-top: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold; text-align: center;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "<p style='text-align:right; font-size:11px'>[[contents](#contents)]</p>\n",
    "\n",
    "# Extra Credit (Optional)<a class=\"anchor\" id=\"ec\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2bdce0cbc50b6e7644bbef9684e4cc7",
     "grade": false,
     "grade_id": "cell-bbbf1b05f86579fd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Q9EC (optional)<a class=\"anchor\" id=\"q9\"></a>\n",
    "\n",
    "### Finding your own similarity metric\n",
    "\n",
    "We've explored using cosine similarity and edit distance to find similar messages to input queries. However, there's a whole world of ways to measure the similarity between two documents. Go forth, and research!\n",
    "\n",
    "(Fun fact: Fundamental information retrieval techniques were in fact developed at Cornell, so you would not be the first Cornellian to disrupt the field)\n",
    "\n",
    "For this question, find a new way of measuring similarity between two documents, and implement a search using your new metric. Your new way of measuring document similarity should be different enough from the two approaches we already implemented. It can be a method you devise or an existing method from somewhere else (make sure to reveal your sources).\n",
    "\n",
    "**Note:** The amount of EC awarded for this question will be determined based on creativity, originality, implementation, and analysis. *Do not delete the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6256ee60a590b4e555a167d189281028",
     "grade": true,
     "grade_id": "extra_credit_2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cs4300-env",
   "language": "python",
   "name": "cs4300-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
